{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.io import read_image\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image \n",
    "import imageio\n",
    "from einops import rearrange \n",
    "from tqdm import tqdm \n",
    "\n",
    "import wandb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "torch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVAE Model\n",
    "\n",
    "### Creating the CVAE model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# CVAE IMPLEMENTATION \n",
    "#######################################################\n",
    "\n",
    "class CVAE(nn.Module): \n",
    "    def __init__(self, input_dim=38_804, h1_dim=1024, h2_dim=512, h3_dim=256, latent_dim=100): \n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder Layers \n",
    "        self.fc1 = nn.Linear(input_dim, h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim, h2_dim)\n",
    "        self.fc3 = nn.Linear(h2_dim, h3_dim)\n",
    "        self.fc41 = nn.Linear(h3_dim, latent_dim)  # mu \n",
    "        self.fc42 = nn.Linear(h3_dim, latent_dim)  # log var \n",
    "\n",
    "        # Decoder Layers \n",
    "        self.fc5 = nn.Linear(latent_dim, h1_dim)\n",
    "        self.fc6 = nn.Linear(h1_dim, h2_dim)\n",
    "        self.fc7 = nn.Linear(h2_dim, input_dim)\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        The encoder portion of the CVAE. \n",
    "\n",
    "        Args: \n",
    "            x (Tensor): Input data of shape [batch_size, 178 x 218], flattened images.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: The mean and log-variance of the approximate posterior \n",
    "            distribution q(z|x), where z is the latent variable. \n",
    "        '''\n",
    "\n",
    "        h1 = F.relu(self.fc1(x)) \n",
    "        h2 = self.fc2(h1)\n",
    "        h3 = F.relu(self.fc3(h2))\n",
    "        mu = self.fc41(h3)\n",
    "        logvar = self.fc42(h3) \n",
    "        \n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Reparametrization trick standard in CVAE's. \n",
    "\n",
    "        Args: \n",
    "            mu: The mean of the approximate posterior distribution: q(z|x)\n",
    "            logvar: The log-variance of the approximate posterior distribution: q(z|x)\n",
    "        \n",
    "        Returns: \n",
    "            z: The latent variable sampled from q(z|x) using the reparametrization trick.\n",
    "            z is size [batch_size, latent_dim], in this case, latent_dim=100.\n",
    "        ''' \n",
    "        \n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        z = mu + std * eps \n",
    "        return z \n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        The decoder portion of CVAE. \n",
    "        Args: \n",
    "            z: The latent variable. \n",
    "\n",
    "        Returns: \n",
    "            x_recon: The reconstructed x sampled from the learned distribution of the decoder. \n",
    "            In this case, x_recon is a reconstructred picture. \n",
    "        '''\n",
    "        h1 = F.relu(self.fc5(z))\n",
    "        h2 = F.relu(self.fc6(h1))\n",
    "        x_recon = F.sigmoid(self.fc7(h2))\n",
    "\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: \n",
    "        \"\"\"\n",
    "        The forward pass on the CVAE. \n",
    "        \"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CVAE TESTING CELL \n",
    "\"\"\"\n",
    " \n",
    "test_x = torch.rand([1, 178 * 218])   # Test \"picture\"\n",
    "#print(test_x)\n",
    "#print(\"Shape:\", test_x.size())\n",
    "\n",
    "# Test the forward pass.\n",
    "cvae = CVAE() \n",
    "x_recon, mu, logvar = cvae.forward(test_x)\n",
    "#print(\"Reconstructed x:\", x_recon)\n",
    "#print(\"Shape of x_recon:\", x_recon.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "\n",
    "### Create a custom dataset class compatible with the torch.DataLoader class\n",
    "### https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "The custom dataset class needs 3 functions: `__init__`, `__len__`, and `__getitem__`. \n",
    "\n",
    "Use an image directory, in my case \"data/CelebA/img_align_celeba/. \n",
    "\n",
    "Use an annotations file, which is the metadata for the image, or the relevant features for the model to know. \n",
    "\n",
    "Notes: \n",
    "    - transforms perform manipulation on data to make it suitable for machine learning models to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebAImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations_file, transform=None, target_transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            annotations_file: The dataset containing all of the relevant labels. \n",
    "            img_dir: The path to the images folder. \n",
    "            transform: Modifies the feature data.\n",
    "            target_transform: Modifies the label data.   \n",
    "        '''\n",
    "        \n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir \n",
    "        self.transform = transform \n",
    "        self.target_transform = target_transform \n",
    "\n",
    "        #self.img_landmarks = pd.read_csv(landmarks_file) for simplicity, i wont use this yet.\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, pd.Series]: \n",
    "        '''\n",
    "        Retrieves the image and its labels of the corresponding index. \n",
    "        '''\n",
    "        # img_dir in this case is ../data/CelebA/img_align_celeba/img_align_celeba\n",
    "        # ie. ../data/CelebA/img_align_celeba/img_align_celeba/000001.jpg\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) \n",
    "        image = read_image(img_path)\n",
    "        labels = self.img_labels.iloc[idx, 1:] # retrieves the vector of labels\n",
    "        labels = torch.tensor(labels) # converts pandas series obj to tensor obj\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            labels = self.target_transform(labels)\n",
    "\n",
    "        return image, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing the dataset class\n",
    "\"\"\"\n",
    "\n",
    "annotations_file = \"../data/CelebA/list_attr_celeba.csv\"\n",
    "img_dir = \"../data/CelebA/img_align_celeba/img_align_celeba\"\n",
    "\n",
    "celeba = CelebAImageDataset(img_dir=img_dir, annotations_file=annotations_file)\n",
    "image, labels = celeba.__getitem__(0) # retrieve image\n",
    "\n",
    "#display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A testing function to display images. \n",
    "\"\"\"\n",
    "\n",
    "def display(image: torch.Tensor): \n",
    "    image_np = image.permute(1, 2, 0).numpy() # convert into numpy array with proper dims\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Manager Class \n",
    "### Responsible for partitioning and creating the dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "class CelebADatasetManager:\n",
    "    def __init__(self, img_dir, annotations_file, transform=None, target_transform=None):\n",
    "        self.dataset = CelebAImageDataset(\n",
    "            img_dir=img_dir,\n",
    "            annotations_file=annotations_file,\n",
    "            transform=transform,\n",
    "            target_transform=target_transform\n",
    "        )\n",
    "\n",
    "        self.split_values = None\n",
    "\n",
    "    def zero_one_normalize(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "    def partition(self, split_value: float):\n",
    "        \"\"\"\n",
    "        Partitions the dataset based on the split_value. ie. 0.6 means 60% train, 40% test.\n",
    "        Args: \n",
    "            split_value: The value that splits the dataset.\n",
    "        Returns: \n",
    "            tuple[torch.Subset, torch.Subset]: Returns the train and test dataset. \n",
    "        \"\"\"\n",
    "        if split_value >= 1 or split_value <= 0:\n",
    "            assert False, \"split_value must be between (0,1)\"\n",
    "        \n",
    "        train_split = split_value \n",
    "        test_split = 1 - split_value\n",
    "\n",
    "        train_idx = round(len(self.dataset) * train_split)\n",
    "        test_idx = round(len(self.dataset) * test_split)\n",
    "\n",
    "        train_dataset = Subset(self.dataset, np.arange(train_idx))\n",
    "        test_dataset = Subset(self.dataset, np.arange(test_idx))\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "\n",
    "    def get_dataloader(self, split_value: float, batch_size: int = 64, shuffle:bool = True):\n",
    "        \"\"\"\n",
    "        Creates train and test dataloaders. \n",
    "\n",
    "        Args: \n",
    "            split_value: The value to split the dataset. ie split_value=0.6 corresponds to 60% train and 40% test. \n",
    "            batch_size: The batch size when doing SGD.\n",
    "            shuffle: Shuffles the dataset after every epoch. \n",
    "        \n",
    "        Returns: \n",
    "            A tuple containing the train and test dataloaders.\n",
    "        \"\"\"\n",
    "        train_dataset, test_dataset = self.partition(split_value)\n",
    "        \n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle= shuffle)\n",
    "        \n",
    "        return train_dataloader, test_dataloader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/s23t762151989wcy_1bx9vqc0000gn/T/ipykernel_36615/2372924361.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(labels) # converts pandas series obj to tensor obj\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, -1, -1,  ..., -1, -1, -1],\n",
       "        [-1, -1,  1,  ..., -1, -1,  1],\n",
       "        [-1, -1, -1,  ..., -1, -1, -1],\n",
       "        ...,\n",
       "        [-1, -1, -1,  ..., -1, -1, -1],\n",
       "        [-1,  1, -1,  ..., -1, -1,  1],\n",
       "        [-1, -1,  1,  ..., -1, -1,  1]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing the dataset manager class.\n",
    "\"\"\"\n",
    "\n",
    "annotations_file = \"../data/CelebA/list_attr_celeba.csv\"\n",
    "img_dir = \"../data/CelebA/img_align_celeba/img_align_celeba\"\n",
    "\n",
    "ds_manager = CelebADatasetManager(img_dir=img_dir, annotations_file=annotations_file)\n",
    "\n",
    "train_dl, test_dl = ds_manager.get_dataloader(split_value=0.6)\n",
    "train_features, train_labels = next(iter(train_dl))\n",
    "\n",
    "train_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-generator-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
