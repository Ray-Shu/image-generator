{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.io import read_image\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image \n",
    "import imageio\n",
    "from einops import rearrange \n",
    "from tqdm import tqdm \n",
    "\n",
    "import wandb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "torch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nt1 = torch.randn((3, 3, 5, 5))\\nc1 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=2, stride=1)\\n\\nflat = nn.Flatten(start_dim=1)\\nt2 = c1(t1)\\nprint(t2.size())\\n\\nflat_t2 = flat(t2)\\nprint(flat_t2.size())\\n'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "t1 = torch.randn((3, 3, 5, 5))\n",
    "c1 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=2, stride=1)\n",
    "\n",
    "flat = nn.Flatten(start_dim=1)\n",
    "t2 = c1(t1)\n",
    "print(t2.size())\n",
    "\n",
    "flat_t2 = flat(t2)\n",
    "print(flat_t2.size())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVAE Model\n",
    "\n",
    "### Creating the CVAE model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# CVAE IMPLEMENTATION \n",
    "#######################################################\n",
    "\n",
    "class CVAE(nn.Module): \n",
    "    def __init__(self, in_channel_1=3, out_channel_1=32, in_channel_2=32, out_channel_2 = 64): \n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder Layers: conv1 -> batchNorm1 -> ReLU1 -> conv2 -> batchNorm2 -> ReLU2 -> Flatten -> FC \n",
    "        self.c1 = nn.Conv2d(in_channel_1, out_channel_2, kernel_size=4)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel_1)\n",
    "        self.r1 = nn.ReLU()\n",
    "        self.c2 = nn.Conv2d(in_channel_2, out_channel_2, kernel_size=4)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel_2)\n",
    "        self.r1 = nn.ReLU()\n",
    "        self.flat = nn.Flatten(start_dim=1) # flatten everything (including color channels)\n",
    "        \n",
    "        \n",
    "        #self.fc1 = nn.Linear()\n",
    "\n",
    "        # Decoder Layers: FC -> unflatten -> convTranspose -> batchNorm -> ReLU -> output\n",
    "\n",
    "    def __get_encoder_output_shape(self, x):\n",
    "        \"\"\"\n",
    "        Runs a forward pass on the encoder portion so I can get the shape for nn.Linear\n",
    "        \"\"\"\n",
    "        x = self.c1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.r1(x) \n",
    "        x = self.c2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.r1(x)\n",
    "        x = self.flat(x) \n",
    "        return x\n",
    "       \n",
    "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        The encoder portion of the CVAE. \n",
    "\n",
    "        Args: \n",
    "            x (Tensor): Input data of shape [batch_size, 178 x 218], flattened images.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: The mean and log-variance of the approximate posterior \n",
    "            distribution q(z|x), where z is the latent variable. \n",
    "        '''\n",
    "\n",
    "        h1 = F.relu(self.fc1(x)) \n",
    "        h2 = self.fc2(h1)\n",
    "        h3 = F.relu(self.fc3(h2))\n",
    "        mu = self.fc41(h3)\n",
    "        logvar = self.fc42(h3) \n",
    "        \n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Reparametrization trick standard in CVAE's. \n",
    "\n",
    "        Args: \n",
    "            mu: The mean of the approximate posterior distribution: q(z|x)\n",
    "            logvar: The log-variance of the approximate posterior distribution: q(z|x)\n",
    "        \n",
    "        Returns: \n",
    "            z: The latent variable sampled from q(z|x) using the reparametrization trick.\n",
    "            z is size [batch_size, latent_dim], in this case, latent_dim=100.\n",
    "        ''' \n",
    "        \n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        z = mu + std * eps \n",
    "        return z \n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        The decoder portion of CVAE. \n",
    "        Args: \n",
    "            z: The latent variable. \n",
    "\n",
    "        Returns: \n",
    "            x_recon: The reconstructed x sampled from the learned distribution of the decoder. \n",
    "            In this case, x_recon is a reconstructred picture. \n",
    "        '''\n",
    "        h1 = F.relu(self.fc5(z))\n",
    "        h2 = F.relu(self.fc6(h1))\n",
    "        x_recon = F.sigmoid(self.fc7(h2))\n",
    "\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: \n",
    "        \"\"\"\n",
    "        The forward pass on the CVAE. \n",
    "        \"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 64 elements not 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#print(test_x)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#print(\"Shape:\", test_x.size())\u001b[39;00m\n\u001b[1;32m     10\u001b[0m cvae \u001b[38;5;241m=\u001b[39m CVAE() \n\u001b[0;32m---> 12\u001b[0m x_recon \u001b[38;5;241m=\u001b[39m \u001b[43mcvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CVAE__get_encoder_output_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_recon\u001b[38;5;241m.\u001b[39msize())\n",
      "Cell \u001b[0;32mIn[198], line 28\u001b[0m, in \u001b[0;36mCVAE.__get_encoder_output_shape\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mRuns a forward pass on the encoder portion so I can get the shape for nn.Linear\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc1(x)\n\u001b[0;32m---> 28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr1(x) \n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc2(x)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/image-generator-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/image-generator-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/image-generator-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/image-generator-env/lib/python3.10/site-packages/torch/nn/functional.py:2822\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2820\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 64 elements not 32"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CVAE TESTING CELL \n",
    "\"\"\"\n",
    "\n",
    "# Tensor in (N, C, H, W)\n",
    "x = torch.zeros([1, 3, 218, 178])   # Test \"picture\"\n",
    "#print(test_x)\n",
    "#print(\"Shape:\", test_x.size())\n",
    "\n",
    "cvae = CVAE() \n",
    "\n",
    "x_recon = cvae._CVAE__get_encoder_output_shape(x)\n",
    "print(x_recon.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Step\n",
    "### Preprocess the CelebA labels such that they are a one-hot encoded vector. \n",
    "\n",
    "After finishing the prototype, we will put this preprocessing part into a separate file in utils/preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_CelebA_labels(input_path:str, output_path:str) -> None: \n",
    "    \"\"\"\n",
    "    Changes the CelebA labels to a one-hot encoding instead of using 1 and -1. \n",
    "\n",
    "    Args: \n",
    "        input_path: The labels csv to read in.\n",
    "        output_path: Create a new csv into this path. \n",
    "    \"\"\"\n",
    "\n",
    "    label_df = pd.read_csv(input_path)\n",
    "    label_df.iloc[:, 1:] = (label_df.iloc[:, 1:] + 1)//2 # converts to one-hot \n",
    "    label_df.to_csv(output_path, index=False)\n",
    "    print(f\"Processed labels saved into {output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing preprocessor  (WORKS!!\n",
    "\n",
    "input = \"../data/CelebA/list_attr_celeba.csv\"\n",
    "output = \"../data/CelebA/preprocessed_labels.csv\"\n",
    "#preprocess_CelebA_labels(input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "\n",
    "### Create a custom dataset class compatible with the torch.DataLoader class\n",
    "### https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "The custom dataset class needs 3 functions: `__init__`, `__len__`, and `__getitem__`. \n",
    "\n",
    "Use an image directory, in my case \"data/CelebA/img_align_celeba/. \n",
    "\n",
    "Use an annotations file, which is the metadata for the image, or the relevant features for the model to know. \n",
    "\n",
    "Notes: \n",
    "    - transforms perform manipulation on data to make it suitable for machine learning models to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebAImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations_file, transform=None, target_transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            annotations_file: The dataset containing all of the relevant labels. \n",
    "            img_dir: The path to the images folder. \n",
    "            transform: Modifies the feature data.\n",
    "            target_transform: Modifies the label data.   \n",
    "        '''\n",
    "        \n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir \n",
    "        self.transform = transform \n",
    "        self.target_transform = target_transform \n",
    "\n",
    "        #self.img_landmarks = pd.read_csv(landmarks_file) for simplicity, i wont use this yet.\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, pd.Series]: \n",
    "        '''\n",
    "        Retrieves the image and its labels of the corresponding index. \n",
    "        '''\n",
    "        # img_dir in this case is ../data/CelebA/img_align_celeba/img_align_celeba\n",
    "        # ie. ../data/CelebA/img_align_celeba/img_align_celeba/000001.jpg\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) \n",
    "        image = read_image(img_path)\n",
    "        labels = self.img_labels.iloc[idx, 1:] # retrieves the vector of labels\n",
    "        labels = torch.tensor(labels) # converts pandas series obj to tensor obj\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            labels = self.target_transform(labels)\n",
    "\n",
    "        return image, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/s23t762151989wcy_1bx9vqc0000gn/T/ipykernel_36615/2372924361.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(labels) # converts pandas series obj to tensor obj\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing the dataset class\n",
    "\"\"\"\n",
    "\n",
    "annotations_file = \"../data/CelebA/preprocessed_labels.csv\"\n",
    "img_dir = \"../data/CelebA/img_align_celeba/img_align_celeba\"\n",
    "\n",
    "celeba = CelebAImageDataset(img_dir=img_dir, annotations_file=annotations_file)\n",
    "image, labels = celeba.__getitem__(0) # retrieve image\n",
    "\n",
    "#display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A testing function to display images. \n",
    "\"\"\"\n",
    "\n",
    "def display(image: torch.Tensor): \n",
    "    image_np = image.permute(1, 2, 0).numpy() # convert into numpy array with proper dims\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Manager Class \n",
    "### Responsible for partitioning and creating the dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "class CelebADatasetManager:\n",
    "    def __init__(self, img_dir, annotations_file, transform=None, target_transform=None):\n",
    "        self.dataset = CelebAImageDataset(\n",
    "            img_dir=img_dir,\n",
    "            annotations_file=annotations_file,\n",
    "            transform=transform,\n",
    "            target_transform=target_transform\n",
    "        )\n",
    "\n",
    "        self.split_values = None\n",
    "\n",
    "    def partition(self, split_value: float):\n",
    "        \"\"\"\n",
    "        Partitions the dataset based on the split_value. ie. 0.6 means 60% train, 40% test.\n",
    "        Args: \n",
    "            split_value: The value that splits the dataset.\n",
    "        Returns: \n",
    "            tuple[torch.Subset, torch.Subset]: Returns the train and test dataset. \n",
    "        \"\"\"\n",
    "        if split_value >= 1 or split_value <= 0:\n",
    "            assert False, \"split_value must be between (0,1)\"\n",
    "        \n",
    "        train_split = split_value \n",
    "        test_split = 1 - split_value\n",
    "\n",
    "        train_idx = round(len(self.dataset) * train_split)\n",
    "        test_idx = round(len(self.dataset) * test_split)\n",
    "\n",
    "        train_dataset = Subset(self.dataset, np.arange(train_idx))\n",
    "        test_dataset = Subset(self.dataset, np.arange(test_idx))\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "\n",
    "    def get_dataloader(self, split_value: float, batch_size: int = 64, shuffle:bool = True):\n",
    "        \"\"\"\n",
    "        Creates train and test dataloaders. \n",
    "\n",
    "        Args: \n",
    "            split_value: The value to split the dataset. ie split_value=0.6 corresponds to 60% train and 40% test. \n",
    "            batch_size: The batch size when doing SGD.\n",
    "            shuffle: Shuffles the dataset after every epoch. \n",
    "        \n",
    "        Returns: \n",
    "            A tuple containing the train and test dataloaders.\n",
    "        \"\"\"\n",
    "        train_dataset, test_dataset = self.partition(split_value)\n",
    "        \n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle= shuffle)\n",
    "        \n",
    "        return train_dataloader, test_dataloader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 218, 178])\n",
      "torch.Size([3, 218, 178])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/s23t762151989wcy_1bx9vqc0000gn/T/ipykernel_36615/2372924361.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(labels) # converts pandas series obj to tensor obj\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing the dataset manager class.\n",
    "\"\"\"\n",
    "\n",
    "annotations_file = \"../data/CelebA/preprocessed_labels.csv\"\n",
    "img_dir = \"../data/CelebA/img_align_celeba/img_align_celeba\"\n",
    "\n",
    "ds_manager = CelebADatasetManager(img_dir=img_dir, annotations_file=annotations_file)\n",
    "\n",
    "train_ds, _ = ds_manager.partition(0.6)\n",
    "image, _ = train_ds[0]\n",
    "\n",
    "train_dl, test_dl = ds_manager.get_dataloader(split_value=0.6)\n",
    "train_features, train_labels = next(iter(train_dl))\n",
    "\n",
    "print(train_features.size())\n",
    "train_labels.size()\n",
    "\n",
    "print(image.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "### Defines a loss function for the CVAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def celbo_loss(x_recon, x, mu, logvar, beta:float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    The ELBO loss used in VAE's / CVAE's.\n",
    "\n",
    "    Args:\n",
    "        x_recon: The reconstructed x created from the CVAE \n",
    "        x: The original x. \n",
    "        mu: The mean of the latent distribution. \n",
    "        logvar: The logvariance of the latent distribution.\n",
    "        beta: The hyperparameter to control how strong the KL loss is. \n",
    "\n",
    "    Returns: \n",
    "        The total loss (scalar). \n",
    "    \"\"\"\n",
    "\n",
    "    # computes the kl divergence per sample. \n",
    "    # for example, with a batch size of 4, we get: [kl_1, kl_2, kl_3, kl_4]\n",
    "    kl = -1/2 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1) \n",
    "    kl = torch.mean(kl, dim=0) # average over batch size\n",
    " \n",
    "    # take ce loss across samples and get the average across all samples \n",
    "    bce = F.binary_cross_entropy(x_recon, x, reduction=\"mean\") \n",
    "\n",
    "    elbo_loss = beta * kl + bce\n",
    "    return elbo_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "### I'm going to create a Trainer class to organize and create modular code for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, train_loader, loss_fn, device = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: The model architecture the trainer will use. \n",
    "            optimizer: The optimizer the trainer will use (adam). \n",
    "            train_loader: The training dataset wrapped into the Pytorch DataLoader iterable object. \n",
    "            loss_fn: The loss function. \n",
    "            epochs: The number of epochs the model will train on.  \n",
    "            device: The device that PyTorch will run calculations under.\n",
    "        \"\"\"\n",
    "        self.model = model \n",
    "        self.optimizer = optimizer \n",
    "        self.train_loader = train_loader \n",
    "        self.loss_fn = loss_fn \n",
    "        self.device = device \n",
    "    \n",
    "    def train(self, epochs=3): \n",
    "        \"\"\"\n",
    "        Trains the model. \n",
    "        \"\"\"\n",
    "        for batch_x, _ in self.train_loader: \n",
    "            print(batch_x.size())\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 218, 178])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/s23t762151989wcy_1bx9vqc0000gn/T/ipykernel_36615/2372924361.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(labels) # converts pandas series obj to tensor obj\n"
     ]
    }
   ],
   "source": [
    "cvae = CVAE()\n",
    "\n",
    "annotations_file = \"../data/CelebA/preprocessed_labels.csv\"\n",
    "img_dir = \"../data/CelebA/img_align_celeba/img_align_celeba\"\n",
    "\n",
    "ds_manager = CelebADatasetManager(img_dir=img_dir, annotations_file=annotations_file)\n",
    "train_dl, _ = ds_manager.get_dataloader(0.5)\n",
    "\n",
    "\n",
    "opt = optim.Adam(cvae.parameters(), lr=1e-3)\n",
    "trainer = Trainer(model=cvae, optimizer=opt, train_loader=train_dl, loss_fn=celbo_loss)\n",
    "trainer.train(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-generator-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
