{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.io import read_image\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image \n",
    "import imageio\n",
    "from einops import rearrange \n",
    "from tqdm import tqdm \n",
    "\n",
    "import wandb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "torch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nt1 = torch.randn((3, 3, 5, 5))\\nc1 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=2, stride=1)\\n\\nflat = nn.Flatten(start_dim=1)\\nt2 = c1(t1)\\nprint(t2.size())\\n\\nflat_t2 = flat(t2)\\nprint(flat_t2.size())\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "t1 = torch.randn((3, 3, 5, 5))\n",
    "c1 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=2, stride=1)\n",
    "\n",
    "flat = nn.Flatten(start_dim=1)\n",
    "t2 = c1(t1)\n",
    "print(t2.size())\n",
    "\n",
    "flat_t2 = flat(t2)\n",
    "print(flat_t2.size())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVAE Model\n",
    "\n",
    "### Creating the CVAE model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# CVAE IMPLEMENTATION \n",
    "#######################################################\n",
    "\n",
    "class CVAE(nn.Module): \n",
    "    def __init__(self, input_channel= 3, c1=32, c2=64, latent_dim=2000): \n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder Layers: conv1 -> batchNorm1 -> ReLU1 -> conv2 -> batchNorm2 -> ReLU2 -> Flatten -> FC \n",
    "        self.c1 = nn.Conv2d(input_channel, c1, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(c1)\n",
    "        self.r1 = nn.ReLU()\n",
    "        self.c2 = nn.Conv2d(c1, c2, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(c2)\n",
    "        self.r2 = nn.ReLU()\n",
    "        self.flat = nn.Flatten(start_dim=1) # flatten everything (including color channels)\n",
    "        \n",
    "        dummy_input = torch.zeros([1, 3, 218, 178]) # celeba picture size\n",
    "        dummy_output = self.__get_encoder_output_shape(dummy_input)\n",
    "        self.fc11 = nn.Linear(dummy_output.shape[1], latent_dim) # gets the mean \n",
    "        self.fc12 = nn.Linear(dummy_output.shape[1], latent_dim) # gets the logvar\n",
    "\n",
    "        # Decoder Layers: FC -> unflatten -> convTranspose -> batchNorm -> ReLU -> output\n",
    "        self.fc2 = nn.Linear(latent_dim, 512 * 4 * 4)  # 512 channels, 4x4 small feature map\n",
    "        self.unflatten = nn.Unflatten(1, (512, 4, 4))\n",
    "        \n",
    "        self.ct1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)  # 8x8\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.r3 = nn.ReLU()\n",
    "        \n",
    "        self.ct2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # 16x16\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.r4 = nn.ReLU()\n",
    "        \n",
    "        self.ct3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)   # 32x32\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        self.r5 = nn.ReLU()\n",
    "\n",
    "        self.ct4 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)    # 64x64\n",
    "        self.bn6 = nn.BatchNorm2d(32)\n",
    "        self.r6 = nn.ReLU() \n",
    "\n",
    "        self.ct5 = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)    # 128x128\n",
    "        self.bn7 = nn.BatchNorm2d(16)\n",
    "        self.r7 = nn.ReLU()\n",
    "\n",
    "        self.ct6 = nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1) # custom kernel to get close to 218x178\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def __get_encoder_output_shape(self, x):\n",
    "        \"\"\"\n",
    "        Runs a forward pass on the encoder portion so I can get the shape for nn.Linear\n",
    "        \"\"\"\n",
    "        x = self.c1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.r1(x) \n",
    "        x = self.c2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.r1(x)\n",
    "        x = self.flat(x) \n",
    "        return x\n",
    "       \n",
    "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        The encoder portion of the CVAE. \n",
    "\n",
    "        Args: \n",
    "            x (Tensor): Input data of shape [batch_size, 178 x 218], flattened images.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: The mean and log-variance of the approximate posterior \n",
    "            distribution q(z|x), where z is the latent variable. \n",
    "        '''\n",
    "        x = self.c1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.r1(x)\n",
    "        x = self.c2(x) \n",
    "        x = self.bn2(x) \n",
    "        x = self.r2(x)\n",
    "        x = self.flat(x) \n",
    "\n",
    "        mu = self.fc11(x)\n",
    "        logvar = self.fc12(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Reparametrization trick standard in CVAE's. \n",
    "\n",
    "        Args: \n",
    "            mu: The mean of the approximate posterior distribution: q(z|x)\n",
    "            logvar: The log-variance of the approximate posterior distribution: q(z|x)\n",
    "        \n",
    "        Returns: \n",
    "            z: The latent variable sampled from q(z|x) using the reparametrization trick.\n",
    "            z is size [batch_size, latent_dim], in this case, latent_dim=100.\n",
    "        ''' \n",
    "        \n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        z = mu + std * eps \n",
    "        return z \n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        The decoder portion of CVAE. \n",
    "        Args: \n",
    "            z: The latent variable. \n",
    "\n",
    "        Returns: \n",
    "            x_recon: The reconstructed x sampled from the learned distribution of the decoder. \n",
    "            In this case, x_recon is a reconstructred picture. \n",
    "        '''\n",
    "        z = self.fc2(z)\n",
    "        z = self.unflatten(z)\n",
    "        \n",
    "        z = self.r3(self.bn3(self.ct1(z)))\n",
    "        z = self.r4(self.bn4(self.ct2(z)))\n",
    "        z = self.r5(self.bn5(self.ct3(z)))\n",
    "        z = self.r6(self.bn6(self.ct4(z)))\n",
    "        z = self.r7(self.bn7(self.ct5(z)))\n",
    "\n",
    "        x_recon = self.sigmoid(self.ct6(z))\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: \n",
    "        \"\"\"\n",
    "        The forward pass on the CVAE. \n",
    "        \"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2000])\n",
      "torch.Size([1, 2000])\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CVAE TESTING CELL \n",
    "\"\"\"\n",
    "\n",
    "# Tensor in (N, C, H, W)\n",
    "x = torch.zeros([1, 3, 218, 178])   # Test \"picture\"\n",
    "#print(test_x)\n",
    "#print(\"Shape:\", test_x.size())\n",
    "\n",
    "cvae = CVAE() \n",
    "\n",
    "mu, logvar = cvae.encode(x)\n",
    "print(mu.size())\n",
    "print(logvar.size())\n",
    "\n",
    "z = cvae.reparametrize(mu, logvar)\n",
    "x_recon = cvae.decode(z)\n",
    "print(x_recon.size())\n",
    "\n",
    "\n",
    "#x_recon = cvae._CVAE__get_encoder_output_shape(x)\n",
    "#x_recon.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Step\n",
    "### Preprocess the CelebA labels such that they are a one-hot encoded vector. \n",
    "\n",
    "After finishing the prototype, we will put this preprocessing part into a separate file in utils/preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_CelebA_labels(input_path:str, output_path:str) -> None: \n",
    "    \"\"\"\n",
    "    Changes the CelebA labels to a one-hot encoding instead of using 1 and -1. \n",
    "\n",
    "    Args: \n",
    "        input_path: The labels csv to read in.\n",
    "        output_path: Create a new csv into this path. \n",
    "    \"\"\"\n",
    "\n",
    "    label_df = pd.read_csv(input_path)\n",
    "    label_df.iloc[:, 1:] = (label_df.iloc[:, 1:] + 1)//2 # converts to one-hot \n",
    "    label_df.to_csv(output_path, index=False)\n",
    "    print(f\"Processed labels saved into {output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing preprocessor  (WORKS!!\n",
    "\n",
    "input = \"../data/CelebA/list_attr_celeba.csv\"\n",
    "output = \"../data/CelebA/preprocessed_labels.csv\"\n",
    "#preprocess_CelebA_labels(input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "\n",
    "### Create a custom dataset class compatible with the torch.DataLoader class\n",
    "### https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "The custom dataset class needs 3 functions: `__init__`, `__len__`, and `__getitem__`. \n",
    "\n",
    "Use an image directory, in my case \"data/CelebA/img_align_celeba/. \n",
    "\n",
    "Use an annotations file, which is the metadata for the image, or the relevant features for the model to know. \n",
    "\n",
    "Notes: \n",
    "    - transforms perform manipulation on data to make it suitable for machine learning models to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebAImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations_file, transform=None, target_transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            annotations_file: The dataset containing all of the relevant labels. \n",
    "            img_dir: The path to the images folder. \n",
    "            transform: Modifies the feature data.\n",
    "            target_transform: Modifies the label data.   \n",
    "        '''\n",
    "        \n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir \n",
    "        self.transform = transform \n",
    "        self.target_transform = target_transform \n",
    "\n",
    "        #self.img_landmarks = pd.read_csv(landmarks_file) for simplicity, i wont use this yet.\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, pd.Series]: \n",
    "        '''\n",
    "        Retrieves the image and its labels of the corresponding index. \n",
    "        '''\n",
    "        # img_dir in this case is ../data/CelebA/img_align_celeba/img_align_celeba\n",
    "        # ie. ../data/CelebA/img_align_celeba/img_align_celeba/000001.jpg\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) \n",
    "        image = read_image(img_path)\n",
    "        labels = self.img_labels.iloc[idx, 1:] # retrieves the vector of labels\n",
    "        labels = torch.tensor(labels) # converts pandas series obj to tensor obj\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            labels = self.target_transform(labels)\n",
    "\n",
    "        return image, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/s23t762151989wcy_1bx9vqc0000gn/T/ipykernel_47015/2372924361.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(labels) # converts pandas series obj to tensor obj\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing the dataset class\n",
    "\"\"\"\n",
    "\n",
    "annotations_file = \"../data/CelebA/preprocessed_labels.csv\"\n",
    "img_dir = \"../data/CelebA/img_align_celeba/img_align_celeba\"\n",
    "\n",
    "celeba = CelebAImageDataset(img_dir=img_dir, annotations_file=annotations_file)\n",
    "image, labels = celeba.__getitem__(0) # retrieve image\n",
    "\n",
    "#display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A testing function to display images. \n",
    "\"\"\"\n",
    "\n",
    "def display(image: torch.Tensor): \n",
    "    image_np = image.permute(1, 2, 0).numpy() # convert into numpy array with proper dims\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Manager Class \n",
    "### Responsible for partitioning and creating the dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "class CelebADatasetManager:\n",
    "    def __init__(self, img_dir, annotations_file, transform=None, target_transform=None):\n",
    "        self.dataset = CelebAImageDataset(\n",
    "            img_dir=img_dir,\n",
    "            annotations_file=annotations_file,\n",
    "            transform=transform,\n",
    "            target_transform=target_transform\n",
    "        )\n",
    "\n",
    "        self.split_values = None\n",
    "\n",
    "    def partition(self, split_value: float):\n",
    "        \"\"\"\n",
    "        Partitions the dataset based on the split_value. ie. 0.6 means 60% train, 40% test.\n",
    "        Args: \n",
    "            split_value: The value that splits the dataset.\n",
    "        Returns: \n",
    "            tuple[torch.Subset, torch.Subset]: Returns the train and test dataset. \n",
    "        \"\"\"\n",
    "        if split_value >= 1 or split_value <= 0:\n",
    "            assert False, \"split_value must be between (0,1)\"\n",
    "        \n",
    "        train_split = split_value \n",
    "        test_split = 1 - split_value\n",
    "\n",
    "        train_idx = round(len(self.dataset) * train_split)\n",
    "        test_idx = round(len(self.dataset) * test_split)\n",
    "\n",
    "        train_dataset = Subset(self.dataset, np.arange(train_idx))\n",
    "        test_dataset = Subset(self.dataset, np.arange(test_idx))\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "\n",
    "    def get_dataloader(self, split_value: float, batch_size: int = 64, shuffle:bool = True):\n",
    "        \"\"\"\n",
    "        Creates train and test dataloaders. \n",
    "\n",
    "        Args: \n",
    "            split_value: The value to split the dataset. ie split_value=0.6 corresponds to 60% train and 40% test. \n",
    "            batch_size: The batch size when doing SGD.\n",
    "            shuffle: Shuffles the dataset after every epoch. \n",
    "        \n",
    "        Returns: \n",
    "            A tuple containing the train and test dataloaders.\n",
    "        \"\"\"\n",
    "        train_dataset, test_dataset = self.partition(split_value)\n",
    "        \n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle= shuffle)\n",
    "        \n",
    "        return train_dataloader, test_dataloader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/s23t762151989wcy_1bx9vqc0000gn/T/ipykernel_47015/2372924361.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(labels) # converts pandas series obj to tensor obj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 218, 178])\n",
      "torch.Size([3, 218, 178])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing the dataset manager class.\n",
    "\"\"\"\n",
    "\n",
    "annotations_file = \"../data/CelebA/preprocessed_labels.csv\"\n",
    "img_dir = \"../data/CelebA/img_align_celeba/img_align_celeba\"\n",
    "\n",
    "ds_manager = CelebADatasetManager(img_dir=img_dir, annotations_file=annotations_file)\n",
    "\n",
    "train_ds, _ = ds_manager.partition(0.6)\n",
    "image, _ = train_ds[0]\n",
    "\n",
    "train_dl, test_dl = ds_manager.get_dataloader(split_value=0.6)\n",
    "train_features, train_labels = next(iter(train_dl))\n",
    "\n",
    "print(train_features.size())\n",
    "train_labels.size()\n",
    "\n",
    "print(image.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/s23t762151989wcy_1bx9vqc0000gn/T/ipykernel_47015/2372924361.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(labels) # converts pandas series obj to tensor obj\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing image reconstruction\n",
    "\"\"\"\n",
    "annotations_file = \"../data/CelebA/preprocessed_labels.csv\"\n",
    "img_dir = \"../data/CelebA/img_align_celeba/img_align_celeba\"\n",
    "\n",
    "dm = CelebADatasetManager(img_dir=img_dir, annotations_file=annotations_file)\n",
    "train_ds, _ = dm.partition(0.6)\n",
    "image, _ = train_ds[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "### Defines a loss function for the CVAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def celbo_loss(x_recon, x, mu, logvar, beta:float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    The ELBO loss used in VAE's / CVAE's.\n",
    "\n",
    "    Args:\n",
    "        x_recon: The reconstructed x created from the CVAE \n",
    "        x: The original x. \n",
    "        mu: The mean of the latent distribution. \n",
    "        logvar: The logvariance of the latent distribution.\n",
    "        beta: The hyperparameter to control how strong the KL loss is. \n",
    "\n",
    "    Returns: \n",
    "        The total loss (scalar). \n",
    "    \"\"\"\n",
    "\n",
    "    # computes the kl divergence per sample. \n",
    "    # for example, with a batch size of 4, we get: [kl_1, kl_2, kl_3, kl_4]\n",
    "    kl = -1/2 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1) \n",
    "    kl = torch.mean(kl, dim=0) # average over batch size\n",
    " \n",
    "    # take ce loss across samples and get the average across all samples \n",
    "    bce = F.binary_cross_entropy(x_recon, x, reduction=\"mean\") \n",
    "\n",
    "    elbo_loss = beta * kl + bce\n",
    "    return elbo_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "### I'm going to create a Trainer class to organize and create modular code for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, train_loader, loss_fn, device = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: The model architecture the trainer will use. \n",
    "            optimizer: The optimizer the trainer will use (adam). \n",
    "            train_loader: The training dataset wrapped into the Pytorch DataLoader iterable object. \n",
    "            loss_fn: The loss function. \n",
    "            epochs: The number of epochs the model will train on.  \n",
    "            device: The device that PyTorch will run calculations under.\n",
    "        \"\"\"\n",
    "        self.model = model \n",
    "        self.optimizer = optimizer \n",
    "        self.train_loader = train_loader \n",
    "        self.loss_fn = loss_fn \n",
    "        self.device = device \n",
    "    \n",
    "    def train(self, epochs=3): \n",
    "        \"\"\"\n",
    "        Trains the model. \n",
    "        \"\"\"\n",
    "        for batch_x, _ in self.train_loader: \n",
    "            print(batch_x.size())\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 218, 178])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/s23t762151989wcy_1bx9vqc0000gn/T/ipykernel_47015/2372924361.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(labels) # converts pandas series obj to tensor obj\n"
     ]
    }
   ],
   "source": [
    "cvae = CVAE()\n",
    "\n",
    "annotations_file = \"../data/CelebA/preprocessed_labels.csv\"\n",
    "img_dir = \"../data/CelebA/img_align_celeba/img_align_celeba\"\n",
    "\n",
    "ds_manager = CelebADatasetManager(img_dir=img_dir, annotations_file=annotations_file)\n",
    "train_dl, _ = ds_manager.get_dataloader(0.5)\n",
    "\n",
    "\n",
    "opt = optim.Adam(cvae.parameters(), lr=1e-3)\n",
    "trainer = Trainer(model=cvae, optimizer=opt, train_loader=train_dl, loss_fn=celbo_loss)\n",
    "trainer.train(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-generator-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
